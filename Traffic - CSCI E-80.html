<!DOCTYPE html>
<!-- saved from url=(0069)https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/ -->
<html lang="en-us"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta property="og:description" content="">

<meta property="og:image" content=""><meta property="og:title" content="Traffic - CSCI E-80">

<meta property="og:url" content=""><link href="https://cs50.harvard.edu/extension/ai/2020/spring/favicon.ico?1586143153" rel="icon">

<!-- https://fontawesome.com/how-to-use/on-the-web/referencing-icons/basic-use -->
<link href="./Traffic - CSCI E-80_files/all.min.css" rel="stylesheet">

<link href="./Traffic - CSCI E-80_files/page.css" rel="stylesheet">

<!-- http://getbootstrap.com/docs/4.3/getting-started/introduction/ -->
<script src="./Traffic - CSCI E-80_files/jquery.min.js"></script>
<script src="./Traffic - CSCI E-80_files/popper.min.js"></script>
<script src="./Traffic - CSCI E-80_files/bootstrap.min.js"></script>

<!-- https://github.com/pellepim/jstimezonedetect -->
<script src="./Traffic - CSCI E-80_files/jstz.min.js"></script>

<!-- https://momentjs.com/, https://momentjs.com/timezone/ -->
<script src="./Traffic - CSCI E-80_files/moment.min.js"></script>
<script src="./Traffic - CSCI E-80_files/moment-timezone-with-data.min.js"></script>



<script src="./Traffic - CSCI E-80_files/jekyll-theme-cs50.js"></script>

<title>Traffic - CSCI E-80</title>
</head>

    <body>

        <div class="container-fluid">

            <div class="row">

                <aside class="col-md">
 
                    <header><h1 data-id="csci-e-80"><a href="https://cs50.harvard.edu/extension/ai/2020/spring/">CSCI E-80</a></h1>

<p class="h5">CS50’s Introduction to<br>
Artificial Intelligence<br>
with Python</p>

<p><a href="https://www.extension.harvard.edu/">Harvard Extension School</a><br>
Spring 2020</p></header>

                    

                    <button aria-controls="nav" aria-expanded="false" class="btn btn-sm collapsed d-md-none" data-target="aside &gt; nav" data-toggle="collapse">
                        Menu
                    </button>

                    <nav class="collapse d-md-block"><hr>

<ul class="fa-ul">
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#announcements">Announcements</a></li>
</ul>

<hr>

<ol start="0">
  <li><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#search">Search</a></li>
  <li><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#knowledge">Knowledge</a></li>
  <li><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#uncertainty">Uncertainty</a></li>
  <li><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#optimization">Optimization</a></li>
  <li><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#learning">Learning</a></li>
  <li><a href="https://cs50.harvard.edu/extension/ai/2020/spring/#neural-networks">Neural Networks</a></li>
</ol>

<hr>

<ul class="fa-ul">
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://vault.cs50.io/1aefc6c7-e592-4657-ac38-3adb9aaebf69">Ed Discussion</a> for Q&amp;A</li>
  <li data-marker="*" class="small"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/quickstart.pdf">Quick Start Guide</a></li>
</ul>

<hr>

<ul class="fa-ul">
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/lectures/">Lectures</a></li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/hours/">Office Hours</a></li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/">Projects</a></li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/sections/">Sections</a></li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/staff/">Staff</a></li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span><a href="https://cs50.harvard.edu/extension/ai/2020/spring/syllabus/">Syllabus</a></li>
</ul></nav>

                    <footer></footer>

                </aside>

                <main class="col-md markdown-body" style="margin-bottom: 352px;">

                    <h1 id="traffic"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#traffic">Traffic</a></h1>

<p>Write an AI to identify which traffic sign appears in a photograph.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ python traffic.py gtsrb
Train on 15984 samples
Epoch 1/10
15984/15984 [==============================] - 10s 623us/sample - loss: 2.8565 - accuracy: 0.3022
Epoch 2/10
15984/15984 [==============================] - 8s 510us/sample - loss: 1.3484 - accuracy: 0.5951
Epoch 3/10
15984/15984 [==============================] - 8s 531us/sample - loss: 0.8283 - accuracy: 0.7494
Epoch 4/10
15984/15984 [==============================] - 12s 736us/sample - loss: 0.5758 - accuracy: 0.8270
Epoch 5/10
15984/15984 [==============================] - 12s 744us/sample - loss: 0.4241 - accuracy: 0.8725
Epoch 6/10
15984/15984 [==============================] - 10s 602us/sample - loss: 0.3391 - accuracy: 0.8956
Epoch 7/10
15984/15984 [==============================] - 10s 620us/sample - loss: 0.3102 - accuracy: 0.9103
Epoch 8/10
15984/15984 [==============================] - 11s 668us/sample - loss: 0.2747 - accuracy: 0.9207
Epoch 9/10
15984/15984 [==============================] - 10s 614us/sample - loss: 0.2208 - accuracy: 0.9362
Epoch 10/10
15984/15984 [==============================] - 8s 528us/sample - loss: 0.1961 - accuracy: 0.9418
10656/10656 - 2s - loss: 0.1392 - accuracy: 0.9606
</code></pre></div></div>

<h2 id="background"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#background">Background</a></h2>

<p>As research continues in the development of self-driving cars, one of the key challenges is <a href="https://en.wikipedia.org/wiki/Computer_vision">computer vision</a>, allowing these cars to develop an understanding of their environment from digital images. In particular, this involves the ability to recognize and distinguish road signs – stop signs, speed limit signs, yield signs, and more.</p>

<p>In this project, you’ll use <a href="https://www.tensorflow.org/">TensorFlow</a> to build a neural network to classify road signs based on an image of those signs. To do so, you’ll need a labeled dataset: a collection of images that have already been categorized by the road sign represented in them.</p>

<p>Several such data sets exist, but for this project, we’ll use the <a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=news">German Traffic Sign Recognition Benchmark</a> (GTSRB) dataset, which contains thousands of images of 43 different kinds of road signs.</p>

<h2 id="getting-started"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#getting-started">Getting Started</a></h2>

<ul class="fa-ul">
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Ensure you have <a href="https://www.python.org/">Python</a> 3.5, 3.6, or 3.7 installed on your computer, as you’ll need to use one of those Python versions for this project. (In particular, note that TensorFlow currently does not support Python 3.8).</li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Download the distribution code from <a href="https://cdn.cs50.net/ai/2020/spring/projects/5/traffic.zip">https://cdn.cs50.net/ai/2020/spring/projects/5/traffic.zip</a> and unzip it.</li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Download the <a href="https://cdn.cs50.net/ai/2020/spring/projects/5/gtsrb.zip">data set</a> for this project and unzip it. Move the resulting <code class="highlighter-rouge">gtsrb</code> directory inside of your <code class="highlighter-rouge">traffic</code> directory.</li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Inside of the <code class="highlighter-rouge">traffic</code> directory, run <code class="highlighter-rouge">pip3 install -r requirements.txt</code> to install this project’s dependencies: <code class="highlighter-rouge">opencv-python</code> for image processing, <code class="highlighter-rouge">scikit-learn</code> for ML-related functions, and <code class="highlighter-rouge">tensorflow</code> for neural networks.</li>
</ul>

<h2 id="understanding"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#understanding">Understanding</a></h2>

<p>First, take a look at the data set by opening the <code class="highlighter-rouge">gtsrb</code> directory. You’ll notice 43 subdirectories in this dataset, numbered <code class="highlighter-rouge">0</code> through <code class="highlighter-rouge">42</code>. Each numbered subdirectory represents a different category (a different type of road sign). Within each traffic sign’s directory is a collection of images of that type of traffic sign.</p>

<p>Next, take a look at <code class="highlighter-rouge">traffic.py</code>. In the <code class="highlighter-rouge">main</code> function, we accept as command-line arguments a directory containing the data and (optionally) a filename to which to save the trained model. The data and corresponding labels are then loaded from the data directory (via the <code class="highlighter-rouge">load_data</code> function) and split into training and testing sets. After that, the <code class="highlighter-rouge">get_model</code> function is called to obtain a compiled neural network that is then fitted on the training data. The model is then evaluated on the testing data. Finally, if a model filename was provided, the trained model is saved to disk.</p>

<p>The <code class="highlighter-rouge">load_data</code> and <code class="highlighter-rouge">get_model</code> functions are left to you to implement.</p>

<h2 id="specification"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#specification">Specification</a></h2>

<p>Complete the implementation of <code class="highlighter-rouge">load_data</code> and <code class="highlighter-rouge">get_model</code> in <code class="highlighter-rouge">traffic.py</code>.</p>

<ul class="fa-ul">
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>The <code class="highlighter-rouge">load_data</code> function should accept as an argument <code class="highlighter-rouge">data_dir</code>, representing the path to a directory where the data is stored, and return image arrays and labels for each image in the data set.
    <ul class="fa-ul">
      <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>You may assume that <code class="highlighter-rouge">data_dir</code> will contain one directory named after each category, numbered <code class="highlighter-rouge">0</code> through <code class="highlighter-rouge">NUM_CATEGORIES - 1</code>. Inside each category directory will be some number of image files.</li>
      <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Use the OpenCV-Python module (<code class="highlighter-rouge">cv2</code>) to read each image as a <code class="highlighter-rouge">numpy.ndarray</code> (a <code class="highlighter-rouge">numpy</code> multidimensional array). To pass these images into a neural network, the images will need to be the same size, so be sure to resize each image to have width <code class="highlighter-rouge">IMG_WIDTH</code> and height <code class="highlighter-rouge">IMG_HEIGHT</code>.</li>
      <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>The function should return a tuple <code class="highlighter-rouge">(images, labels)</code>. <code class="highlighter-rouge">images</code> should be a list of all of the images in the data set, where each image is represented as a <code class="highlighter-rouge">numpy.ndarray</code> of the appropriate size. <code class="highlighter-rouge">labels</code> should be a list of integers, representing the category number for each of the corresponding images in the <code class="highlighter-rouge">images</code> list.</li>
    </ul>
  </li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>The <code class="highlighter-rouge">get_model</code> function should return a compiled neural network model.
    <ul class="fa-ul">
      <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>You may assume that the input to the neural network will be of the shape <code class="highlighter-rouge">(IMG_WIDTH, IMG_HEIGHT, 3)</code> (that is, an array representing an image of width <code class="highlighter-rouge">IMG_WIDTH</code>, height <code class="highlighter-rouge">IMG_HEIGHT</code>, and <code class="highlighter-rouge">3</code> values for each pixel for red, green, and blue).</li>
      <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>The output layer of the neural network should have <code class="highlighter-rouge">NUM_CATEGORIES</code> units, one for each of the traffic sign categories.</li>
      <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>The number of layers and the types of layers you include in between are up to you. You may wish to experiment with:
        <ul class="fa-ul">
          <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>different numbers of convolutional and pooling layers</li>
          <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>different numbers and sizes of filters for convolutional layers</li>
          <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>different pool sizes for pooling layers</li>
          <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>different numbers and sizes of hidden layers</li>
          <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>dropout</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>Ultimately, much of this project is about exploring documentation and investigating different options in <code class="highlighter-rouge">cv2</code> and <code class="highlighter-rouge">tensorflow</code> and seeing what results you get when you try them!</p>

<p>You should not modify anything else in <code class="highlighter-rouge">traffic.py</code> other than the functions the specification calls for you to implement, though you may write additional functions and/or import other Python standard library modules. You may also import <code class="highlighter-rouge">numpy</code> or <code class="highlighter-rouge">pandas</code>, if familiar with them, but you should not use any other third-party Python modules. You may modify the global variables defined at the top of the file to test your program with other values.</p>

<h2 id="hints"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#hints">Hints</a></h2>

<ul class="fa-ul">
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Check out the official <a href="https://www.tensorflow.org/guide/keras/overview">Tensorflow Keras overview</a> for some guidelines for the syntax of building neural network layers. You may find the lecture source code useful as well.</li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>The <a href="https://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_tutorials.html">OpenCV-Python</a> documentation may prove helpful for reading images as arrays and then resizing them.</li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>Once you’ve resized an image <code class="highlighter-rouge">img</code>, you can verify its dimensions by printing the value of <code class="highlighter-rouge">img.shape</code>. If you’ve resized the image correctly, its shape should be <code class="highlighter-rouge">(30, 30, 3)</code> (assuming <code class="highlighter-rouge">IMG_WIDTH</code> and <code class="highlighter-rouge">IMG_HEIGHT</code> are both <code class="highlighter-rouge">30</code>).</li>
  <li data-marker="*"><span class="fa-li"><i class="fas fa-circle"></i></span>If you’d like to practice with a smaller data set, you can download a <a href="https://cdn.cs50.net/ai/2020/spring/projects/5/gtsrb-small.zip">modified dataset</a> that contains only 3 different types of road signs instead of 43.</li>
</ul>

<h2 id="how-to-submit"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#how-to-submit">How to Submit</a></h2>

<p>If you don’t already have it installed, install <code class="highlighter-rouge">submit50</code> by running <code class="highlighter-rouge">pip3 install submit50</code>. Then, execute the below, logging in with your GitHub username and password when prompted. For security, you’ll see asterisks (<code class="highlighter-rouge">*</code>) instead of the actual characters in your password.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>submit50 ai50/problems/2020/spring/traffic
</code></pre></div></div>

<h2 id="acknowledgements"><a data-id="" href="https://cs50.harvard.edu/extension/ai/2020/spring/projects/5/traffic/#acknowledgements">Acknowledgements</a></h2>

<p>Data provided by <a href="http://benchmark.ini.rub.de/index.php?section=gtsrb&amp;subsection=dataset#Acknowledgements">J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. The German Traffic Sign Recognition Benchmark: A multi-class classification competition. In Proceedings of the IEEE International Joint Conference on Neural Networks, pages 1453–1460. 2011</a></p>


                </main>

            </div>

        </div>

        
 
    


</body></html>